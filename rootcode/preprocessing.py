"""Spatial processing functions.

The :meth:`execute` function in this module performs the entire workflow of converting
spatial inputs to ROOT to the input tables required for the optimization step.
"""
import os
import sys
import shutil
import math
import tempfile
import collections
import glob
import json
import shapely.wkb
import shapely.prepared

from osgeo import ogr
from osgeo import gdal
from osgeo import osr
import numpy as np
import pandas as pd

import pygeoprocessing
from . import arith_parser as ap


class RootPreprocessingError(Exception):
    pass



def execute(args):
    """Encapsulate optimization workflow.

    This algorithm creates an SDU grid, a marginal gain raster,
    aggregate those value to the grid, create a table for an IP
    optimizer, and invokes that optimizer.

    Parameters:
        args (dict): must contain the following fields:

            * activity_masks: *tbd*
            * baseline_file: *tbd*
            * cell_size: *tbd*
            * combined_factors: *tbd*
            * csv_output_folder: *tbd*
            * grid_type: *tbd**
            * mask_raster: *tbd*
            * raster_li: *tbd*
            * raster_table: *tbd*
            * sdu_id_col: *tbd*
            * serviceshed_list: *tbd*
            * workspace: *tbd*

            raster_list (list): list of paths to single band raster paths on
                disk that represent per-pixel marginal values.  These rasters
                must be the same size and in geospatial projection.
            value_names (list): a parallel list of `raster_list` representing
                the well known names of the raster paths in `raster_list`.
                These names are later used by the optimizer for objectives
                and constraints.
            raster_table (RasterTable): class holding paths to rasters indexed
                by activity and factor names: raster_table[a][f] -> path.
                Has attributes activity_names and factor_names containing lists
                of each naming type taken from the raster table file.
            mask_raster (string): path to a single band raster that indicates
                pixels of interest.  This raster must be the same size and
                be in the same geospatial projection as those in
                `raster_list`.  Any nodata pixels in this list mask out all
                values given in `raster_list`.
            workspace (string): path to a directory to use as a workspace.
                NOT SURE WHERE THIS IS USED
            output_folder (string): path to output files. This will include
                at least the generated spatial decision unit grid. ANY OTHERS?
                NOTE IN EXAMPLE IT'S LISTED TWICE.
            grid_type (string): one of 'square' or 'hexagon' which is used to
                indicate the shape of the spatial decision grid generated by
                this worksflow.
            cell_size (int/float): indicates the cell size of the spatial
                decision unit grid in the projected linear units of the
                `mask_raster`.  If `grid_type` is 'square', then this
                indicates the side length of the grid.  If `hexagon`,
                indicates minor axis length.
            csv_output_folder (string): destination folder for the output csvs.
                Individual files will be named after their activity name. 
            baseline_file (string): name of target baseline file which is a
                copy of `csv_filepath` except all marginal values are set to
                0.
            sdu_id_col (string): desired name of the column generated in
                `csv_filepath` which represents the spatial decision unit ID.

    Returns:
        None.
    """
    print('BEGINNING PREPROCESSING')

    # print('CHECK: Ensuring inputs in same projection and same size.')
    # _assert_inputs_same_size_and_srs(args['raster_list'] + [args['mask_raster']],
    #                                  args['serviceshed_list'])

    print('STEP: Constructing output paths')
    f_reg = {
        'sdu_grid': os.path.join(args['workspace'], 'sdu_grid.shp'),
        'table_folder': args['csv_output_folder'],
        'baseline_ip_table': os.path.join(
            args['csv_output_folder'], args['baseline_file']),
        }
    for path in f_reg.values():
        dir_path = os.path.dirname(path)
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)

    # Create merged activity mask
    mask_path_list = args['activity_masks'].values()
    all_activity_mask = os.path.join(args['workspace'], 'all_activity_mask.tif')
    _create_overlapping_activity_mask(mask_path_list, all_activity_mask)

    # Creating or copying SDU shapefile
    if args['grid_type'] in ['square', 'hexagon']:
        print('STEP: Creating grid')
        _grid_vector_across_raster(
            all_activity_mask, args['grid_type'], args['cell_size'],
            f_reg['sdu_grid'], args['sdu_id_col'])
    elif os.path.isfile(args['grid_type']):
        _copy_sdu_file(args['grid_type'], f_reg['sdu_grid'])
    else:
        print('invalid SDU grid argument: {}'.format(args['grid_type']))
    # TODO: do we want to clean up the polygons either way? or only for a regular grid?
    print('STEP: Removing polygons that only cover nodata')
    _remove_nonoverlapping_sdus(
        f_reg['sdu_grid'], all_activity_mask, args['sdu_id_col'])

    # Calculate serviceshed coverage and average values
    print('STEP: Calculate SDU serviceshed coverage')
    if 'serviceshed_list' in args and args['serviceshed_list'] is not None:
        sdu_serviceshed_coverage = _serviceshed_coverage(
            f_reg['sdu_grid'], args['sdu_id_col'], args["serviceshed_list"],
            args["serviceshed_names"], args["serviceshed_values"])
    else:
        sdu_serviceshed_coverage = None

    # activity-specific processing section
    raster_table = args['raster_table']
    activities = raster_table.activity_names

    for activity in activities:
        print('aggregating rasters for {}'.format(activity))
        table_file = os.path.join(args['csv_output_folder'], activity+'.csv')
        mask_path = args['activity_masks'][activity]

        # raster cleaning
        print('INTERMEDIATE: cleaning marginal value rasters with large '
              'and corrupt negative nodata')
        clean_raster_file_lookup = {}
        print('raster_table[activity]: {}'.format(raster_table[activity]))
        for mv_id, mv_path in raster_table[activity].items():
            f_reg[mv_id] = os.path.join(args['workspace'], os.path.basename(mv_path))
            _clean_negative_nodata_values(mv_path, f_reg[mv_id])
            clean_raster_file_lookup[mv_id] = f_reg[mv_id]

        print('STEP: Aggregate Value Rasters to SDU')
        marginal_value_lookup = _aggregate_marginal_values(
            f_reg['sdu_grid'], args['sdu_id_col'], mask_path, clean_raster_file_lookup)

        print('STEP: Create IP table')
        _build_ip_table(
            args['sdu_id_col'], activities, activity, marginal_value_lookup,
            sdu_serviceshed_coverage, table_file)

        print('Step: Add combined factors')
        if 'combined_factors' in args and args['combined_factors'] is not None:
            _add_combined_factors(table_file, args['combined_factors'])

    print('STEP: Create baseline table')
    baseline_value_lookup = marginal_value_lookup.copy()
    # zero out all the marginal values since that's equivalent of baseline
    for marginal_value_tuple in baseline_value_lookup.values():
        for mv_id in marginal_value_tuple[1]:
            marginal_value_tuple[1][mv_id] = [0.0, 0, 0.0]

    _build_ip_table(
        args['sdu_id_col'], activities, None, baseline_value_lookup, sdu_serviceshed_coverage,
        f_reg['baseline_ip_table'], baseline_table=True)
    if 'combined_factors' in args and args['combined_factors'] is not None:
        _add_combined_factors(f_reg['baseline_ip_table'], args['combined_factors'])


def _serviceshed_coverage(
        sdu_grid_path, sdu_id_fieldname, serviceshed_path_list,
        serviceshed_id_list, serviceshed_values):
    """Construct serviceshed SDU coverage lookup.

    Calculates sum of SDU coverage over all polygons in the serviceshed path.

    Parameters:
        sdu_grid_path (string): path to a single layer SDU polygon vector
            that has a key field id named `sdu_id_fieldname`.
        sdu_id_fieldname (string): fieldname in the `sdu_grid_path` that
            indicates the integer ID of each SDU.
        serviceshed_path_list (string): list of paths to serviceshed polygon
            vectors to aggregate over.
        serviceshed_id_list (string): parallel list to `serviceshed_path_list`
            indicating the desired ID strings to index serviceshed polygon
            overlap per SDU.
        serviceshed_values (dict): a dictionary indexed by serviceshed ids
            defined in serviceshed_id_list.  If present, value is a list of
            serviceshed features that should be multiplied by sdu/serviceshed
            overlap proportion, and summed in dictionary.  See return value
            for example.


    Returns:
        a dictionary of the form:
            {
                sdu_id_0: {
                    "serviceshed_id_a":
                        (serviceshed coverage proportion for a on id_0,
                         {service_shed_a_value_i: sum of value_i multiplied
                          by proportion of coverage of sdu_id_0 with
                          servicshed _id_a.})
                    "serviceshed_id_b": ....
                },
                sdu_id_1: {....
            }
    """
    sdu_vector = ogr.Open(sdu_grid_path)
    sdu_layer = sdu_vector.GetLayer()

    sdu_lookup = {}
    for sdu_feature in sdu_layer:
        sdu_lookup[sdu_feature.GetField(str(sdu_id_fieldname))] = (
            shapely.wkb.loads(sdu_feature.GetGeometryRef().ExportToWkb()))
    sdu_layer = None
    sdu_vector = None

    result = collections.defaultdict(
        lambda: collections.defaultdict(
            lambda: [0.0, collections.defaultdict(float)]))


    for serviceshed_id, serviceshed_path in zip(
            serviceshed_id_list, serviceshed_path_list):
        print('serviceshed_id: {}'.format(serviceshed_id))
        print('serviceshed_values[serviceshed_id]: {}'.format(serviceshed_values[serviceshed_id]))
        serviceshed_vector = ogr.Open(serviceshed_path)
        for serviceshed_layer in serviceshed_vector:
            for serviceshed_feature in serviceshed_layer:
                serviceshed_geometry = serviceshed_feature.GetGeometryRef()
                serviceshed_polygon = shapely.wkb.loads(
                    serviceshed_geometry.ExportToWkb())
                prep_serviceshed_polygon = shapely.prepared.prep(
                    serviceshed_polygon)
                for sdu_id, sdu_poly in sdu_lookup.items():
                    if not prep_serviceshed_polygon.intersects(sdu_poly):
                        # define some placeholders just in case so when we
                        # create a flat table later, everything is defined
                        # everywhere.
                        if serviceshed_id not in result[sdu_id]:
                            result[sdu_id][serviceshed_id][0] = 0.0
                            if serviceshed_id in serviceshed_values:
                                for value_id in serviceshed_values[serviceshed_id]:
                                    result[sdu_id][serviceshed_id][1][value_id] = 0.0
                        continue

                    sdu_intersection = sdu_poly.intersection(
                        serviceshed_polygon)
                    coverage_proportion = (
                        sdu_intersection.area / sdu_poly.area)
                    result[sdu_id][serviceshed_id][0] += (
                        coverage_proportion)
                    if serviceshed_id in serviceshed_values:
                        for value_id in serviceshed_values[serviceshed_id]:
                            if serviceshed_feature.GetField(str(value_id)) is None:
                                raise ValueError('shapefile contains NULL or missing values\n\t{}\nin field {}'.format(
                                    serviceshed_path, str(value_id)
                                ))
                            result[sdu_id][serviceshed_id][1][value_id] += (
                                coverage_proportion *
                                serviceshed_feature.GetField(
                                    str(value_id)))
    return result


def _build_ip_table(
        sdu_col_name, activity_list, activity_name, marginal_value_lookup,
        sdu_serviceshed_coverage, target_ip_table_path, baseline_table=False):
    """Build a table for Integer Programmer.

    Output is a CSV table with columns identifying the aggregating SDU_ID,
    stats about SDU and mask coverage, as well as aggregate values for
    marginal values.

    Parameters:
        sdu_col_name (string): desired name of the SDU id column in the
            target IP table.
        marginal_value_lookup (dict): in pseudocode:
         { sdu_id0:
                (sdu area, sdu pixel coverage, mask pixel count,
                 mask pixel coverage in Ha),
                {marginal value id a: (
                    aggreated values, n pixels of coverage,
                    aggregated value per Ha of covrage),
                 marginal value id b: ...},
              sdu_id1: ...
            }
        sdu_serviceshed_coverage (dict): in pseudocode:
            {
                sdu_id_0: {
                    "serviceshed_id_a":
                        [serviceshed coverage proportion for a on id_0,
                         {service_shed_a_value_i: sum of value_i multiplied
                          by proportion of coverage of sdu_id_0 with
                          servicshed _id_a.}]
                    "serviceshed_id_b": ....
                },
                sdu_id_1: {....
            }
        target_ip_table_path (string): path to target IP table that will
            have the columns:
                SDU_ID,pixel_count,area_ha,maskpixct,maskpixha,mv_ida,mv_ida_perHA
    """
    if activity_name is not None:
        try:
            activity_index = activity_list.index(activity_name)
        except ValueError:
            msg = 'activity_name not found in activity_list in _build_ip_table'
            raise RootPreprocessingError(msg)
    else:
        activity_index = None

    with open(target_ip_table_path, 'w') as target_ip_file:
        # write header
        target_ip_file.write(
            "{},pixel_count,area_ha".format(sdu_col_name))
        target_ip_file.write(",%s_ha" * len(activity_list) % tuple(activity_list))
        target_ip_file.write(',exclude')
        # target_ip_file.write(
        #     "{},pixel_count,area_ha,{}_px,{}_ha".format(
        #         sdu_col_name, activity_name, activity_name))
        # This gets the "first" value in the dict, then the keys of that dict
        # also makes sense to sort them so it's easy to navigate the CSV.
        marginal_value_ids = sorted(
            marginal_value_lookup.values().next()[1].keys())
        n_mv_ids = len(marginal_value_ids)
        target_ip_file.write((",%s" * n_mv_ids) % tuple(marginal_value_ids))
        # target_ip_file.write(
        #     (",%s_perHA" * n_mv_ids) % tuple(marginal_value_ids))
        if sdu_serviceshed_coverage is not None:
            first_serviceshed_lookup = sdu_serviceshed_coverage.values().next()
        else:
            first_serviceshed_lookup = {}
        serviceshed_ids = sorted(first_serviceshed_lookup.keys())
        target_ip_file.write(
            (",%s" * len(serviceshed_ids)) % tuple(serviceshed_ids))
        value_ids = {
            sid: sorted(first_serviceshed_lookup[sid][1].keys()) for
            sid in serviceshed_ids
            }
        for serviceshed_id in serviceshed_ids:
            for value_id in value_ids[serviceshed_id]:
                target_ip_file.write(",%s_%s" % (serviceshed_id, value_id))
        target_ip_file.write('\n')

        # write each row
        for sdu_id in sorted(marginal_value_lookup):
            # id, pixel count, total pixel area,
            target_ip_file.write(
                "%d,%d,%f" % (
                    sdu_id, marginal_value_lookup[sdu_id][0][1],
                    marginal_value_lookup[sdu_id][0][0]))

            # areas by activity
            areas = [0 for _ in range(len(activity_list))]
            if baseline_table is False and activity_index is not None:
                areas[activity_index] = marginal_value_lookup[sdu_id][0][3]
            target_ip_file.write(",%f" * len(areas) % tuple(areas))
            # if all areas are 0, that means in particular the current activity has 0 available area
            # and we want to exclude this SDU as an option
            if baseline_table is False and max(areas) == 0:
                target_ip_file.write(',1')
            else:
                target_ip_file.write(',0')

            # write out all the marginal value aggregate values
            for mv_id in marginal_value_ids:
                target_ip_file.write(
                    ",%f" % marginal_value_lookup[sdu_id][1][mv_id][0])
            # write out all marginal value aggregate values per Ha
            # for mv_id in marginal_value_ids:
            #     target_ip_file.write(
            #         ",%f" % marginal_value_lookup[sdu_id][1][mv_id][2])
            # serviceshed values
            for serviceshed_id in serviceshed_ids:
                target_ip_file.write(
                    (",%f" % sdu_serviceshed_coverage[sdu_id][serviceshed_id][0]))
            for serviceshed_id in serviceshed_ids:
                for value_id in value_ids[serviceshed_id]:
                    target_ip_file.write(
                        (",%f" % sdu_serviceshed_coverage[sdu_id][serviceshed_id][1][value_id]))
            target_ip_file.write('\n')


def _add_combined_factors(data_table_path, combined_factors_dict):
    """
    Opens data_table_path as dataframe, then for each name and list of factors in 
    combined_factors_dict, creates a new column 'name' with the value multiplying 
    the columns in the CF dict value.
    
    :param data_table_path: 
    :param combined_factors_dict: 
    :return: 
    """
    df = pd.read_csv(data_table_path)

    for col_name, factors in combined_factors_dict.items():
        df[col_name] = ap.apply(df, factors)

    df.to_csv(data_table_path, index=False)


def _clean_negative_nodata_values(
        base_raster_path, target_clean_raster_path):
    """Reset large negative corrupt nodata values to valid ones.

    Parameters:
        base_raster_path (string): path to a single band floating point
            raster with a large negative nodata value and pixel values that
            might also  be nodata but are corrupt from roundoff error.
        target_clean_raster_path (string): path to desired target raster that
            will ensure the nodata value is a ffinto(float32).min and any
            values in the source raster that are close to that value are set
            to this.

    Returns:
        None.
    """
    target_nodata = np.finfo(np.float32).min
    base_nodata = pygeoprocessing.get_nodata_from_uri(base_raster_path)
    if base_nodata > target_nodata / 10:
        print(
            "Base raster doesn't have a large nodata value; it's likely"
            " not one of the corrupt float32.min rasters we were dealing"
            " with.  Copying %s to %s without modification.") % (
                base_raster_path, target_clean_raster_path)
        shutil.copy(base_raster_path, target_clean_raster_path)

    pygeoprocessing.new_raster_from_base_uri(
        base_raster_path, target_clean_raster_path, 'GTiff',
        target_nodata, gdal.GDT_Float32)
    target_raster = gdal.Open(target_clean_raster_path, gdal.GA_Update)
    target_band = target_raster.GetRasterBand(1)

    for block_offset, data_block in pygeoprocessing.iterblocks(
            base_raster_path):
        possible_nodata_mask = data_block < (target_nodata / 10)
        data_block[possible_nodata_mask] = target_nodata
        target_band.WriteArray(
            data_block, xoff=block_offset['xoff'], yoff=block_offset['yoff'])


def _aggregate_marginal_values(
        sdu_grid_path, sdu_key_id, mask_raster_path, marginal_value_lookup):
    """Build table that indexes SDU ids with aggregated marginal values.

    Parameters:
        sdu_grid_path (string): path to single layer polygon vector with
            integer field id that uniquely identifies each polygon.
        sdu_key_id (string): field in `sdu_grid_path` that uniquely identifies
            each feature.
        mask_raster_path (string): path to a mask raster whose pixels are
            considered "valid" if they are not nodata.
        marginal_value_lookup (dict): keys are marginal value IDs that
            will be used in the optimization table; values are paths to
            single band rasters.

    Returns:
        A dictionary that encapsulates stats about each polygon, mask coverage
        and marginal value aggregation and coverage. Each key in the dict is
        the SDU_ID for a polygon, while the value is a tuple that contains
        first polygon/mask stats, then another dict for marginal value stats.
        In pseudocode:
            { sdu_id0:
                (sdu area, sdu pixel coverage, mask pixel count,
                 mask pixel coverage in Ha),
                {marginal value id a: (
                    aggregated values, n pixels of coverage,
                    aggregated value per Ha of coverage),
                 marginal valud id b: ...},
              sdu_id1: ...
            }
    """
    # TODO: drop activity mask, get activity from the rasters - require nodata for non-transition pixels

    print('marginal_value_lookup: {}'.format(marginal_value_lookup))
    marginal_value_ids = marginal_value_lookup.keys()
    with tempfile.NamedTemporaryFile(dir='.', delete=False) as id_raster_file:
        id_raster_path = id_raster_file.name

    id_nodata = -1
    pygeoprocessing.new_raster_from_base_uri(
        marginal_value_lookup[marginal_value_ids[0]], id_raster_path, 'GTiff',
        id_nodata, gdal.GDT_Int32, fill_value=id_nodata)
    id_raster = gdal.Open(id_raster_path, gdal.GA_Update)
    mask_raster = gdal.Open(mask_raster_path)
    mask_band = mask_raster.GetRasterBand(1)
    mask_nodata = mask_band.GetNoDataValue()
    geotransform = mask_raster.GetGeoTransform()
    # note: i'm assuming square pixels that are aligned NS and EW and
    # projected in meters as linear units
    pixel_area_m2 = float((geotransform[1]) ** 2)

    vector = ogr.Open(sdu_grid_path, 1)  # open for reading
    layer = vector.GetLayer()
    gdal.RasterizeLayer(
        id_raster, [1], layer, options=['ATTRIBUTE=%s' % sdu_key_id])
    id_raster = None
    marginal_value_rasters = [
        gdal.Open(marginal_value_lookup[marginal_value_id])
        for marginal_value_id in marginal_value_ids]
    marginal_value_bands = [
        raster.GetRasterBand(1) for raster in marginal_value_rasters]
    marginal_value_nodata_list = [
        band.GetNoDataValue() for band in marginal_value_bands]

    # first element in tuple is the coverage stats:
    # (sdu area, sdu pixel count, mask pixel count, mask pixel coverage in Ha)
    # second element 3 element list (aggregate sum, pixel count, sum/Ha)
    marginal_value_sums = collections.defaultdict(
        lambda: (
            [0.0, 0, 0, 0.0],
            dict((mv_id, [0.0, 0, None]) for mv_id in marginal_value_ids)))

    # format of sdu_coverage is:
    # (sdu area, sdu pixel count, mask pixel count, mask pixel coverage in Ha)
    for block_offset, id_block in pygeoprocessing.iterblocks(id_raster_path):
        marginal_value_blocks = [
            band.ReadAsArray(**block_offset) for band in marginal_value_bands]
        mask_block = mask_band.ReadAsArray(**block_offset)
        for aggregate_id in np.unique(id_block):
            if aggregate_id == id_nodata:
                continue
            aggregate_mask = id_block == aggregate_id
            # update sdu pixel coverage
            # marginal_value_sums[aggregate_id][0] =
            #    (sdu area, sdu pixel count, mask pixel count, mask pixel Ha)
            marginal_value_sums[aggregate_id][0][1] += np.count_nonzero(
                aggregate_mask)
            valid_mask_block = mask_block[aggregate_mask]
            marginal_value_sums[aggregate_id][0][2] += np.count_nonzero(
                valid_mask_block != mask_nodata)
            for mv_id, mv_nodata, mv_block in zip(
                    marginal_value_ids, marginal_value_nodata_list,
                    marginal_value_blocks):
                valid_mv_block = mv_block[aggregate_mask]
                # raw aggregation of marginal value
                # marginal_value_sums[aggregate_id][1][mv_id] =
                # (sum, pixel count, pixel Ha)
                marginal_value_sums[aggregate_id][1][mv_id][0] += np.nansum(
                    valid_mv_block[np.logical_and(
                        valid_mv_block != mv_nodata,
                        valid_mask_block != mask_nodata)])
                # pixel count coverage of marginal value
                marginal_value_sums[aggregate_id][1][mv_id][1] += (
                    np.count_nonzero(np.logical_and(
                        valid_mv_block != mv_nodata,
                        valid_mask_block != mask_nodata)))
    # calculate SDU, mask coverage in Ha, and marginal value Ha coverage
    for sdu_id in marginal_value_sums:
        marginal_value_sums[sdu_id][0][0] = (
            marginal_value_sums[sdu_id][0][1] * pixel_area_m2 / 10000.0)
        marginal_value_sums[sdu_id][0][3] = (
            marginal_value_sums[sdu_id][0][2] * pixel_area_m2 / 10000.0)
        # calculate the 3rd tuple of marginal value per Ha
        for mv_id in marginal_value_sums[sdu_id][1]:
            if marginal_value_sums[sdu_id][1][mv_id][1] != 0:
                marginal_value_sums[sdu_id][1][mv_id][2] = (
                    marginal_value_sums[sdu_id][1][mv_id][0] / (
                        marginal_value_sums[sdu_id][1][mv_id][1] *
                        pixel_area_m2 / 10000.0))
            else:
                marginal_value_sums[sdu_id][1][mv_id][2] = 0.0
    del marginal_value_bands[:]
    del marginal_value_rasters[:]
    mask_band = None
    mask_raster = None
    os.remove(id_raster_path)
    return marginal_value_sums


def _remove_nonoverlapping_sdus(vector_path, mask_raster_path, key_id_field):
    """Remove polygons in `vector_path` that don't overlap valid data.

    Parameters:
        vector_path (string): path to a single layer polygon shapefile
            that has a  unique key field named `key_id_field`.  This function
            modifies this polygon to remove any polygons.
        make_raster_path (string): path to a mask raster; polygons in
            `vector_path` that only overlap nodata pixels will be removed.
        key_id_field (string): name of key id field in the polygon vector.

    Returns:
        None.
    """
    with tempfile.NamedTemporaryFile(dir='.', delete=False) as id_raster_file:
        id_raster_path = id_raster_file.name

    pygeoprocessing.new_raster_from_base_uri(
        mask_raster_path, id_raster_path, 'GTiff', -1,
        gdal.GDT_Int32, fill_value=-1)
    id_raster = gdal.Open(id_raster_path, gdal.GA_Update)

    tmp_vector_dir = tempfile.mkdtemp()
    vector_basename = os.path.basename(vector_path)
    vector_driver = ogr.GetDriverByName("ESRI Shapefile")
    base_vector = ogr.Open(vector_path)
    vector = vector_driver.CopyDataSource(
        base_vector, os.path.join(tmp_vector_dir, vector_basename))
    base_vector = None
    layer = vector.GetLayer()

    gdal.RasterizeLayer(
        id_raster, [1], layer, options=['ATTRIBUTE=%s' % key_id_field])
    id_band = id_raster.GetRasterBand(1)
    mask_nodata = pygeoprocessing.get_nodata_from_uri(mask_raster_path)
    covered_ids = set()
    for mask_offset, mask_block in pygeoprocessing.iterblocks(
            mask_raster_path):
        id_block = id_band.ReadAsArray(**mask_offset)
        valid_mask = mask_block != mask_nodata
        covered_ids.update(np.unique(id_block[valid_mask]))

    # cleanup the ID raster since we're done with it
    id_band = None
    id_raster = None
    os.remove(id_raster_path)

    # now it's sufficient to check if the min value on each feature is defined
    # if so there are valid pixels underneath, otherwise none.
    for feature in layer:
        feature_id = feature.GetField(str(key_id_field))
        if feature_id not in covered_ids:
            layer.DeleteFeature(feature.GetFID())

    print('INFO: Packing Target SDU Grid')
    # remove target vector and create a new one in its place with same layer
    # and fields
    os.remove(vector_path)
    target_vector = vector_driver.CreateDataSource(vector_path)
    spatial_ref = osr.SpatialReference(layer.GetSpatialRef().ExportToWkt())
    target_layer = target_vector.CreateLayer(
        str(os.path.splitext(vector_basename)[0]),
        spatial_ref, ogr.wkbPolygon)
    layer_defn = layer.GetLayerDefn()
    for index in range(layer_defn.GetFieldCount()):
        field_defn = layer_defn.GetFieldDefn(index)
        field_defn.SetWidth(24)
        target_layer.CreateField(field_defn)

    # copy over undeleted features
    layer.ResetReading()
    for feature in layer:
        target_layer.CreateFeature(feature)
    target_layer = None
    target_vector = None
    layer = None
    vector = None

    # remove unpacked vector
    shutil.rmtree(tmp_vector_dir)


def _grid_vector_across_raster(
        mask_raster_path, grid_type, cell_size, out_grid_vector_path,
        sdu_id_fieldname):
    """Convert vector to a regular grid.

    Here the vector is gridded such that all cells are contained within the
    original vector.  Cells that would intersect with the boundary are not
    produced.

    Parameters:
        mask_raster_path (string): path to a single band raster where
            pixels valued at '1' are valid and invalid otherwise.
        grid_type (string): one of "square" or "hexagon"
        cell_size (float): dimensions of the grid cell in the projected units
            of `vector_path`; if "square" then this indicates the side length,
            if "hexagon" indicates the width of the horizontal axis.
        out_grid_vector_path (string): path to the output ESRI shapefile
            vector that contains a gridded version of `vector_path`, this file
            should not exist before this call
        sdu_id_fieldname (string): desired key id field

    Returns:
        None
    """
    driver = ogr.GetDriverByName('ESRI Shapefile')
    if os.path.exists(out_grid_vector_path):
        driver.DeleteDataSource(out_grid_vector_path)

    raster_mask = gdal.Open(mask_raster_path)
    spatial_reference = osr.SpatialReference(raster_mask.GetProjection())

    out_grid_vector = driver.CreateDataSource(out_grid_vector_path)
    grid_layer = out_grid_vector.CreateLayer(
        'grid', spatial_reference, ogr.wkbPolygon)
    grid_layer.CreateField(
        ogr.FieldDefn(str(sdu_id_fieldname), ogr.OFTInteger))
    grid_layer_defn = grid_layer.GetLayerDefn()

    geotransform = raster_mask.GetGeoTransform()
    # minx maxx miny maxy
    extent = [
        geotransform[0],
        (geotransform[0] +
         raster_mask.RasterXSize * geotransform[1] +
         raster_mask.RasterYSize * geotransform[2]),
        (geotransform[3] +
         raster_mask.RasterXSize * geotransform[4] +
         raster_mask.RasterYSize * geotransform[5]),
        geotransform[3]
        ]
    raster_mask = None

    # flip around if one direciton is negative or not; annoying case that'll
    # always linger unless directly approached like this
    extent = [
        min(extent[0], extent[1]),
        max(extent[0], extent[1]),
        min(extent[2], extent[3]),
        max(extent[2], extent[3])]

    if grid_type == 'hexagon':
        # calculate the inner dimensions of the hexagons
        grid_width = extent[1] - extent[0]
        grid_height = extent[3] - extent[2]
        delta_short_x = cell_size * 0.25
        delta_long_x = cell_size * 0.5
        delta_y = cell_size * 0.25 * (3 ** 0.5)

        # Since the grid is hexagonal it's not obvious how many rows and
        # columns there should be just based on the number of squares that
        # could fit into it.  The solution is to calculate the width and
        # height of the largest row and column.
        n_cols = int(math.floor(grid_width / (3 * delta_long_x)) + 1)
        n_rows = int(math.floor(grid_height / delta_y) + 1)

        def _generate_polygon(col_index, row_index):
            """Generate a points for a closed hexagon."""
            if (row_index + 1) % 2:
                centroid = (
                    extent[0] + (delta_long_x * (1 + (3 * col_index))),
                    extent[2] + (delta_y * (row_index + 1)))
            else:
                centroid = (
                    extent[0] + (delta_long_x * (2.5 + (3 * col_index))),
                    extent[2] + (delta_y * (row_index + 1)))
            x_coordinate, y_coordinate = centroid
            hexagon = [(x_coordinate - delta_long_x, y_coordinate),
                       (x_coordinate - delta_short_x, y_coordinate + delta_y),
                       (x_coordinate + delta_short_x, y_coordinate + delta_y),
                       (x_coordinate + delta_long_x, y_coordinate),
                       (x_coordinate + delta_short_x, y_coordinate - delta_y),
                       (x_coordinate - delta_short_x, y_coordinate - delta_y),
                       (x_coordinate - delta_long_x, y_coordinate)]
            return hexagon
    elif grid_type == 'square':
        def _generate_polygon(col_index, row_index):
            """Generate points for a closed square."""
            square = [
                (extent[0] + col_index * cell_size + x,
                 extent[2] + row_index * cell_size + y)
                for x, y in [
                    (0, 0), (cell_size, 0), (cell_size, cell_size),
                    (0, cell_size), (0, 0)]]
            return square
        n_rows = int((extent[3] - extent[2]) / cell_size)
        n_cols = int((extent[1] - extent[0]) / cell_size)
    else:
        raise ValueError('Unknown polygon type: %s' % grid_type)

    for row_index in range(n_rows):
        for col_index in range(n_cols):
            polygon_points = _generate_polygon(col_index, row_index)
            ring = ogr.Geometry(ogr.wkbLinearRing)
            for xoff, yoff in polygon_points:
                ring.AddPoint(xoff, yoff)
            poly = ogr.Geometry(ogr.wkbPolygon)
            poly.AddGeometry(ring)

            poly_feature = ogr.Feature(grid_layer_defn)
            poly_feature.SetGeometry(poly)
            poly_feature.SetField(
                str(sdu_id_fieldname), row_index * n_cols + col_index)
            grid_layer.CreateFeature(poly_feature)
    grid_layer.SyncToDisk()


def _assert_inputs_same_size_and_srs(raster_path_list, shapefile_path_list):
    """Assert all rasters in list have same dimensions and geoprojection.

    Raises a ValueError if any of the rasters in the list are of a different
    size or geospatial projection from each other.

    Parameter:
        raster_path_list (list): list of paths to rasters.

    Returns:
        None.
    """
    raster_list = [gdal.Open(raster_path) for raster_path in raster_path_list]
    raster_projections = []
    raster_geotransforms = []

    for raster in raster_list:
        projection_as_str = raster.GetProjection()
        raster_sr = osr.SpatialReference()
        raster_sr.ImportFromWkt(projection_as_str)
        raster_projections.append((raster_sr, raster.GetFileList()[0]))

        raster_geotransforms.append(raster.GetGeoTransform())

    for index in range(len(raster_projections)-1):
        if not raster_projections[index][0].IsSame(
                raster_projections[index+1][0]):
            raise ValueError(
                "These two rasters might not be in the same projection."
                " The different projections are:\n\n'filename: %s'\n%s\n\n"
                "and:\n\n'filename:%s'\n%s\n\n",
                raster_projections[index][1],
                raster_projections[index][0].ExportToPrettyWkt(),
                raster_projections[index+1][1],
                raster_projections[index+1][0].ExportToPrettyWkt())

        if not (raster_geotransforms[index][1] == raster_geotransforms[index+1][1] and
                raster_geotransforms[index][5] == raster_geotransforms[index+1][5]):
            raise ValueError("These rasters might not have the same cell size: \n\t{}\n\t{}".format(
                raster_path_list[index], raster_path_list[index+1]
            ))

        raster_spatial_extents_conditions = [
            raster_geotransforms[index][0] == raster_geotransforms[index + 1][0],
            raster_geotransforms[index][3] == raster_geotransforms[index + 1][3],
            raster_list[index].RasterXSize == raster_list[index + 1].RasterXSize,
            raster_list[index].RasterYSize == raster_list[index + 1].RasterYSize
        ]
        if not all(raster_spatial_extents_conditions):
            raise ValueError("These rasters might not have identical spatial extents {} : \n\t{}\n\t{}".format(
                raster_spatial_extents_conditions, raster_path_list[index], raster_path_list[index+1]
            ))

    shapefile_projections = []
    for shapefile in shapefile_path_list:
        driver = ogr.GetDriverByName('ESRI Shapefile')
        dataset = driver.Open(shapefile)
        layer = dataset.GetLayer()
        spatial_ref = layer.GetSpatialRef()

        vector_sr = osr.SpatialReference()
        vector_sr.ImportFromWkt(spatial_ref.ExportToWkt())
        shapefile_projections.append(vector_sr)

    raster_ref = raster_projections[0][0]
    for index in range(len(shapefile_projections)):
        if not raster_ref.IsSame(shapefile_projections[index]):
            raise ValueError(
                'The projection of this shapefile does not match raster projections: \n\t{}'.format(
                    shapefile_path_list[index]
                )
            )

    del raster_list[:]
    # del shapefile_list[:]


def _copy_sdu_file(source, dest):
    """
    Copies the shapefile, renaming it to sdu_grid.shp
    :param source: 
    :param dest: 
    :return: 
    """
    shp_base = os.path.splitext(source)[0]
    shp_components = glob.glob(shp_base+'.*')
    dest_no_ext = os.path.splitext(dest)[0]
    for f in shp_components:
        ext = os.path.splitext(f)[1]
        new_f = os.path.join(dest_no_ext + ext)
        shutil.copy(f, new_f)


def _create_overlapping_activity_mask(mask_path_list, target_file,
                                      reference_mask=0):

    # TODO: get nodata vals from each mask_path, make comparison against those instead of np.nan
    # need them as closure in this function, zip with vals to line up

    mask_nodatas = [pygeoprocessing.get_nodata_from_uri(mask_path) for mask_path in mask_path_list]
    ref_path = mask_path_list[reference_mask]
    ref_dtype = pygeoprocessing.get_datatype_from_uri(ref_path)
    ref_nodata = pygeoprocessing.get_nodata_from_uri(ref_path)
    pixel_size = pygeoprocessing.get_cell_size_from_uri(ref_path)

    def pixel_op(*vals):
        if all([x == nodata for x, nodata in zip(vals, mask_nodatas)]):
            return ref_nodata
        else:
            return 1

    pygeoprocessing.vectorize_datasets(
        mask_path_list,
        pixel_op,
        target_file,
        ref_dtype,
        ref_nodata,
        pixel_size,
        bounding_box_mode='dataset',
        dataset_to_align_index=reference_mask,
        dataset_to_bound_index=reference_mask,
    )


if __name__ == '__main__':
    # LOOK HERE FOR AN EXAMPLE OF HOW TO RUN
    if len(sys.argv) == 2:
        JSON_ARGS_PATH = sys.argv[1]
    else:
        JSON_ARGS_PATH = (
            r"E:\Dropbox\optimization_data\banana_rich.json")
    execute(JSON_ARGS_PATH)
